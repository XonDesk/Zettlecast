# .antigravity.md

This file provides context to **Antigravity** when working with code in this repository.

## Overview

Zettlecast is a local-first AI knowledge system with semantic search, automatic link suggestions, and immutable identity. It's a digital Zettelkasten middleware built with FastAPI, LanceDB, and Next.js.

## Development Environment

- **Python**: 3.11 or 3.12 (NeMo requires this; 3.13+ not supported)
- **Virtual Environment**: `.venv`
- **Package Manager**: pip with `pyproject.toml`

### Setup Commands

```bash
# macOS/Linux
./setup.sh

# Windows
.\setup.ps1
```

### Run the Application

```bash
# macOS/Linux
./run.sh

# Windows
.\.venv\Scripts\Activate.ps1
zettlecast serve
```

**Servers:**
- API: http://localhost:8000
- Frontend: http://localhost:3000

## Project Structure

```
src/zettlecast/
├── main.py          # FastAPI app and endpoints
├── config.py        # Settings from .env (pydantic-settings)
├── db.py            # LanceDB operations
├── parser.py        # PDF/Web/Audio parsing
├── chunker.py       # Text splitting (512 chars, 50 overlap)
├── search.py        # Vector search + reranking
├── identity.py      # UUID and frontmatter management
├── models.py        # Pydantic models
├── cli.py           # CLI commands
└── podcast/         # Audio transcription module
    ├── transcriber_factory.py  # Platform-aware backend selection
    ├── base_transcriber.py     # Abstract interface
    ├── mac_transcriber.py      # parakeet-mlx + pyannote
    ├── whisper_transcriber.py  # faster-whisper fallback
    ├── container_transcriber.py # Docker NeMo (Windows)
    ├── nemo_transcriber.py     # Direct NeMo integration
    ├── aligner.py              # Word-to-speaker alignment
    ├── vad.py                  # Voice Activity Detection
    ├── enhancer.py             # LLM transcript enhancement
    ├── queue.py                # Batch processing queue
    └── models.py               # Podcast data models
```

## CLI Commands

```bash
zettlecast ingest /path/to/files  # Ingest documents
zettlecast add https://example.com # Quick-add URL
zettlecast search "query" -k 5    # Search notes
zettlecast token                  # Get API token
zettlecast stats                  # Database statistics
zettlecast serve                  # Start API server

# Podcast commands
zettlecast podcast add /path/to/audio
zettlecast podcast run
zettlecast podcast status
```

## Testing

```bash
pytest                                    # Run all tests
pytest tests/test_chunker.py              # Specific file
pytest --cov=src/zettlecast               # With coverage
ruff check src/ tests/                    # Lint
ruff format src/ tests/                   # Format
mypy src/zettlecast                       # Type check
```

## Architecture

### Vector Database (LanceDB)

**Notes Table:**
- `uuid` (str): Primary identifier
- `title` (str): Note title
- `source_type`: pdf | web | audio | markdown | rss
- `full_text` (str): Complete text (auto-embedded)
- `vector` (Vector[1024]): EmbeddingGemma-300M
- `content_hash` (str): SHA256 for deduplication
- `status`: inbox | reviewed | archived

### Processing Pipeline

1. **Ingestion**: Parse → UUID → Hash check → Chunk → Store → Create markdown
2. **Search**: Query → Vector search (top 50) → Rerank → Return top K
3. **Link Suggestions**: Note text → Similar notes → Rerank → Filter rejections

### Audio Transcription Backends

| Backend | Platform | Transcription | Diarization | Setup |
|---------|----------|---------------|-------------|-------|
| **parakeet-mlx** | Mac M-series | ✅ Fast | pyannote.audio | `pip install -e ".[mac]"` |
| **NeMo Container** | Windows+GPU | ✅ Parallel | MSDD | Docker |
| **faster-whisper** | Any | ✅ Reliable | ❌ None | Default |

**TranscriberFactory** auto-selects the best backend for the platform.

## Configuration

**`.env` file** (key settings):

```env
# Storage
STORAGE_PATH=~/_BRAIN_STORAGE
LANCEDB_PATH=~/_BRAIN_STORAGE/.lancedb

# Embeddings
EMBEDDING_MODEL=google/embeddinggemma-300m
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# Audio Transcription
ASR_BACKEND=auto           # auto, parakeet-mlx, nemo, whisper
DIARIZATION_BACKEND=auto   # auto, pyannote, nemo, none
WHISPER_MODEL=large-v3-turbo
HF_TOKEN=                  # Required for pyannote diarization

# LLM
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama3.2:3b
```

## Important Patterns

### UUID Management
- Every note MUST have a UUID in frontmatter
- UUIDs are immutable (content-independent)
- Use `identity.py` for frontmatter operations

### Deduplication
- SHA256 content hash prevents duplicates
- Check `db.get_note_by_hash()` before creating

### PDF Parsing (Tiered)
1. pypdf (fast)
2. Marker (if chars_per_page < 500)
3. Docling (optional, heavy)

### Rejection Tracking
- User rejections stored in `rejected_edges` table
- Always filter when showing suggestions

### Error Handling
- Parsers return `ProcessingResult` with status: success | partial | failed
- Failed ingestions don't crash the server

## NeMo Troubleshooting

**Common issues:**
- Deadlocks: Processing is sequential to avoid
- Missing device key: Added to diarization config
- Model loading: Falls back to faster-whisper

**Recommendations:**
- Production: Use Whisper (default)
- Test NeMo with small files first
- Windows: Prefer Whisper unless diarization needed
